from ultralytics.yolo.engine.predictor import BasePredictor
from ultralytics.yolo.engine.results import Results
from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, SETTINGS, callbacks, ops
from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box
from ultralytics.yolo.utils.torch_utils import smart_inference_mode
from ultralytics.yolo.utils.files import increment_path
from ultralytics.yolo.utils.checks import check_imshow
from ultralytics.yolo.cfg import get_cfg
from PySide6.QtWidgets import QApplication, QMainWindow, QFileDialog, QMenu
from PySide6.QtGui import QImage, QPixmap, QColor
from PySide6.QtCore import QTimer, QThread, Signal, QObject, QPoint, Qt

from lprr import CHARS
from ui.CustomMessageBox import MessageBox
from ui.home import Ui_MainWindow
from UIFunctions import *
from collections import defaultdict
from pathlib import Path
from utils.capnums import Camera
from utils.rtsp_win import Window
from PIL import Image
from lprr.plate import de_lpr,dr_plate
import numpy as np
import time
import json
import torch
import sys
import cv2
import os


class YoloPredictor(BasePredictor, QObject):
    yolo2main_pre_img = Signal(np.ndarray)  # raw image signal
    yolo2main_res_img = Signal(np.ndarray)  # test result signal
    yolo2main_status_msg = Signal(str)  # Detecting/pausing/stopping/testing complete/error reporting signal
    yolo2main_fps = Signal(str)  # fps
    yolo2main_labels = Signal(dict)  # Detected target results (number of each category)
    yolo2main_progress = Signal(int)  # Completeness
    yolo2main_class_num = Signal(int)  # Number of categories detected
    yolo2main_target_num = Signal(int)  # Targets detected

    def __init__(self, cfg=DEFAULT_CFG, overrides=None):

        # å¤šé‡ç»§æ‰¿
        super(YoloPredictor, self).__init__()
        QObject.__init__(self)

        # get_cfg() å‡½æ•°å¯ä»¥ä»é…ç½®æ–‡ä»¶ä¸­è·å–å‚æ•° ã€ä½¿ç”¨ self.args ä¿å­˜æœ€ç»ˆçš„é…ç½®ã€‘
        self.args = get_cfg(cfg, overrides)


        project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task
        # print(project)
        # print(Path(SETTINGS['runs_dir']))
        name = f'{self.args.mode}'
        self.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)

        self.done_warmup = False
        if self.args.show:
            self.args.show = check_imshow(warn=True)

        # æˆ‘å®šä¹‰çš„å±æ€§
        self.iscamera = 0
        self.capture_frame = None

        # GUI args
        self.used_model_name = None  # The detection model name to use
        self.new_model_name = None  # Models that change in real time
        self.source = ''  # input source
        self.stop_dtc = False  # Termination detection
        self.continue_dtc = True  # pause
        self.save_res = False  # Save test results
        self.save_txt = False  # save label(txt) file
        self.iou_thres = 0.45  # iou
        self.conf_thres = 0.25  # conf
        self.speed_thres = 10  # delay, ms
        self.labels_dict = {}  # return a dictionary of results
        self.progress_value = 0  # progress bar

        # Usable if setup is done
        self.model = None
        self.data = self.args.data  # data_dict
        self.imgsz = None
        self.device = None
        self.dataset = None
        self.vid_path, self.vid_writer = None, None
        self.annotator = None
        self.data_path = None
        self.source_type = None
        self.batch = None
        self.callbacks = defaultdict(list, callbacks.default_callbacks)  # add callbacks
        callbacks.add_integration_callbacks(self)

    ####################################è‡ªå®šä¹‰

    @smart_inference_mode()
    def camera_run(self):

        try:
            if self.args.verbose:
                LOGGER.info('')

            # set model
            self.yolo2main_status_msg.emit('Loding Model...')
            print('åŠ è½½æ¨¡å‹')

            if not self.model:
                self.setup_model(self.new_model_name)
                self.used_model_name = self.new_model_name

            # Check save path/label
            print('æ£€æŸ¥ä¿å­˜è·¯å¾„/æ ‡ç­¾')
            if self.save_res or self.save_txt:
                (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            self.seen, self.windows, self.dt, self.batch = 0, [], (ops.Profile(), ops.Profile(), ops.Profile()), None
            # åˆ›å»ºåä¸º dt çš„å®ä¾‹å˜é‡ï¼Œç”¨äºå­˜å‚¨ä¸€ä¸ªå…ƒç»„ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºåŒ…å«ä¸‰ä¸ªå¯¹è±¡ ops.Profile() çš„å…ƒç»„ã€‚
            # ops.Profile() æ˜¯æŒ‡ä» ops æ¨¡å—ä¸­å¯¼å…¥åä¸º Profile() çš„å¯¹è±¡ã€‚

            print('start detection')
            # start detection

            count = 0  # run location frame
            start_time = time.time()  # used to calculate the frame rate

            # ------------------
            self.capture = cv2.VideoCapture(0)  # åˆ›å»ºä¸€ä¸ª OpenCV è§†é¢‘æ•è·å¯¹è±¡

            while True:
                # time.sleep(0.06)  # ä¼‘çœ  60 æ¯«ç§’ï¼ˆ0.06 ç§’ï¼‰
                ret, frame = self.capture.read()  # æ•è·æ‘„åƒå¤´çš„å›¾åƒ
                if ret:  # å¦‚æœè¯»å–æˆåŠŸ
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    self.capture_frame = Image.fromarray(frame_rgb)

                # print('è®¾ç½®èµ„æºæ¨¡å¼')
                print(self.capture_frame)
                self.setup_source(self.capture_frame)

                # warmup model
                # çƒ­èº«æ¨¡å‹
                if not self.done_warmup:
                    # è°ƒç”¨æ¨¡å‹çš„ warmup å‡½æ•°ï¼Œå…¶ä¸­ imgsz å‚æ•°ä¸ºè¾“å…¥å›¾åƒçš„å¤§å°
                    # å¦‚æœæ¨¡å‹ä½¿ç”¨ PyTorchï¼Œimgsz å‚æ•°åº”ä¸º [batch_size, channels, height, width]
                    # å¦‚æœæ¨¡å‹ä½¿ç”¨ Tritonï¼Œimgsz å‚æ•°åº”ä¸º [height, width, channels, batch_size]
                    self.model.warmup(
                        imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
                    # å°† done_warmup æ ‡è®°ä¸º Trueï¼Œä»¥æ ‡è®°æ¨¡å‹å·²ç»çƒ­èº«è¿‡
                    self.done_warmup = True
                    print('çƒ­èº«å®Œæ¯•')

                batch = iter(self.dataset)

                # pause switch  ç”¨äºæ§åˆ¶ç¨‹åºçš„æš‚åœå’Œç»§ç»­
                if self.continue_dtc:
                    # time.sleep(0.001)
                    self.yolo2main_status_msg.emit('Detecting...')
                    batch = next(self.dataset)  # next data

                    self.batch = batch
                    path, im, im0s, vid_cap, s = batch
                    visualize = increment_path(self.save_dir / Path(path).stem,
                                               mkdir=True) if self.args.visualize else False

                    # Calculation completion and frame rate (to be optimized)
                    count += 1  # frame count +1
                    all_count = 1000  # all_count å¯ä»¥è°ƒæ•´ï¼ï¼ï¼
                    self.progress_value = int(count / all_count * 1000)  # progress bar(0~1000)
                    if count % 5 == 0 and count >= 5:  # Calculate the frame rate every 5 frames
                        self.yolo2main_fps.emit(str(int(5 / (time.time() - start_time))))
                        start_time = time.time()

                    # preprocess
                    # self.dt åŒ…å«äº†ä¸‰ä¸ª DetectorTime ç±»å‹çš„å¯¹è±¡ï¼Œè¡¨ç¤ºé¢„å¤„ç†ã€æ¨ç†å’Œåå¤„ç†æ‰€èŠ±è´¹çš„æ—¶é—´

                    ## ä½¿ç”¨ with è¯­å¥è®°å½•ä¸‹ä¸‹ä¸€è¡Œä»£ç æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œself.dt[0] è¡¨ç¤ºè®°å½•é¢„å¤„ç†æ“ä½œæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚
                    print('preprocess...')
                    with self.dt[0]:
                        # è°ƒç”¨ self.preprocess æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å¤„ç†åçš„å›¾åƒèµ‹å€¼ç»™ im å˜é‡ã€‚
                        im = self.preprocess(im)
                        # å¦‚æœ im çš„ç»´åº¦ä¸º 3ï¼ˆRGB å›¾åƒï¼‰ï¼Œåˆ™è¡¨ç¤ºè¿™æ˜¯ä¸€å¼ å•å¼ å›¾åƒï¼Œéœ€è¦å°†å…¶æ‰©å±•æˆ 4 ç»´ï¼ŒåŠ ä¸Š batch ç»´åº¦ã€‚
                        if len(im.shape) == 3:
                            im = im[None]  # expand for batch dim  æ‰©å¤§æ‰¹é‡è°ƒæš—
                    # inference
                    with self.dt[1]:
                        # è°ƒç”¨æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œæ¨ç†ï¼Œå¹¶å°†ç»“æœèµ‹å€¼ç»™ preds å˜é‡ã€‚
                        preds = self.model(im, augment=self.args.augment, visualize=visualize)
                    # postprocess
                    with self.dt[2]:
                        # è°ƒç”¨ self.postprocess æ–¹æ³•å¯¹æ¨ç†ç»“æœè¿›è¡Œåå¤„ç†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° self.results å˜é‡ä¸­ã€‚
                        # å…¶ä¸­ preds æ˜¯æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œim æ˜¯æ¨¡å‹è¾“å…¥çš„å›¾åƒï¼Œè€Œ im0s æ˜¯åŸå§‹å›¾åƒçš„å¤§å°ã€‚
                        self.results = self.postprocess(preds, im, im0s)

                    # visualize, save, write results
                    print('visualize, save, write results...')
                    n = len(im)  # To be improved: support multiple img


                    for i in range(n):
                        self.results[i].speed = {
                            'preprocess': self.dt[0].dt * 1E3 / n,
                            'inference': self.dt[1].dt * 1E3 / n,
                            'postprocess': self.dt[2].dt * 1E3 / n
                        }
                        p, im0 = (path[i], im0s[i].copy()) if self.source_type.webcam or self.source_type.from_img \
                            else (path, im0s.copy())

                        p = Path(p)  # the source dir

                        # s:::   video 1/1 (6/6557) 'path':
                        # must, to get boxs\labels
                        label_str = self.write_results(i, self.results, (p, im, im0))  # labels   /// original :s +=

                        # labels and nums dict
                        class_nums = 0
                        target_nums = 0
                        self.labels_dict = {}
                        if 'no detections' in label_str:
                            pass
                        else:
                            for ii in label_str.split(',')[:-1]:

                                nums, label_name = ii.split('~')

                                li = nums.split(':')[-1]

                                print(li)

                                self.labels_dict[label_name] = int(li)
                                target_nums += int(li)
                                class_nums += 1

                        # save img or video result
                        if self.save_res:
                            self.save_preds(vid_cap, i, str(self.save_dir / p.name))

                        # Send test results ã€å‘é€ä¿¡å· ç»™ label æ˜¾ç¤ºå›¾åƒã€‘
                        self.yolo2main_res_img.emit(im0)  # after detection  ----------ç»“æœ
                        self.yolo2main_pre_img.emit(im0s if isinstance(im0s, np.ndarray) else im0s[0])  # Before testing
                        # self.yolo2main_labels.emit(self.labels_dict)        # webcam need to change the def write_results
                        self.yolo2main_class_num.emit(class_nums)
                        self.yolo2main_target_num.emit(target_nums)

                        print('send success!')

                        if self.speed_thres != 0:
                            time.sleep(self.speed_thres / 1000)  # delay , ms åŸå‡½æ•°

                    self.yolo2main_progress.emit(self.progress_value)  # progress bar

                # Detection completed
                if count + 1 >= all_count:
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # release final video writer
                    self.yolo2main_status_msg.emit('Detection completed')
                    break

        except Exception as e:

            self.capture.release()  # é‡Šæ”¾è§†é¢‘è®¾å¤‡
            print('error:', e)
            self.yolo2main_status_msg.emit('%s' % e)

    ##################################è‡ªå®šä¹‰ ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

    # main for detect
    @smart_inference_mode()
    def run(self):
        try:

            if self.args.verbose:
                LOGGER.info('')

            # set model
            self.yolo2main_status_msg.emit('Loding Model...')
            print('åŠ è½½æ¨¡å‹')

            if not self.model:
                self.setup_model(self.new_model_name)
                self.used_model_name = self.new_model_name

            # set source  [è§†é¢‘èµ„æº]
            print('è®¾ç½®èµ„æºæ¨¡å¼')
            self.setup_source(self.source if self.source is not None else self.args.source)

            # Check save path/label
            print('æ£€æŸ¥ä¿å­˜è·¯å¾„/æ ‡ç­¾')
            if self.save_res or self.save_txt:
                (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            # warmup model
            # çƒ­èº«æ¨¡å‹
            if not self.done_warmup:
                # è°ƒç”¨æ¨¡å‹çš„ warmup å‡½æ•°ï¼Œå…¶ä¸­ imgsz å‚æ•°ä¸ºè¾“å…¥å›¾åƒçš„å¤§å°
                # å¦‚æœæ¨¡å‹ä½¿ç”¨ PyTorchï¼Œimgsz å‚æ•°åº”ä¸º [batch_size, channels, height, width]
                # å¦‚æœæ¨¡å‹ä½¿ç”¨ Tritonï¼Œimgsz å‚æ•°åº”ä¸º [height, width, channels, batch_size]
                self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
                # å°† done_warmup æ ‡è®°ä¸º Trueï¼Œä»¥æ ‡è®°æ¨¡å‹å·²ç»çƒ­èº«è¿‡
                self.done_warmup = True
            print('çƒ­èº«å®Œæ¯•')

            self.seen, self.windows, self.dt, self.batch = 0, [], (ops.Profile(), ops.Profile(), ops.Profile()), None
            # åˆ›å»ºåä¸º dt çš„å®ä¾‹å˜é‡ï¼Œç”¨äºå­˜å‚¨ä¸€ä¸ªå…ƒç»„ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºåŒ…å«ä¸‰ä¸ªå¯¹è±¡ ops.Profile() çš„å…ƒç»„ã€‚
            # ops.Profile() æ˜¯æŒ‡ä» ops æ¨¡å—ä¸­å¯¼å…¥åä¸º Profile() çš„å¯¹è±¡ã€‚

            print('start detection')
            # start detection
            # for batch in self.dataset:

            count = 0  # run location frame
            start_time = time.time()  # used to calculate the frame rate
            batch = iter(self.dataset)
            while True:
                # Termination detection  ã€ç»ˆæ­¢æ£€æµ‹ã€‘
                if self.stop_dtc:
                    # é‡Šæ”¾CV2è§†é¢‘å†™å…¥å™¨
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # release final video writer
                    self.yolo2main_status_msg.emit('Detection terminated!')
                    break

                # Change the model midway ã€åˆ‡æ¢modelã€‘  å¦‚æœä¸ç›¸ç­‰ï¼Œåˆ™æ‰§è¡Œ setup_model() æ–¹æ³•è®¾ç½®æ–°çš„æ¨¡å‹
                if self.used_model_name != self.new_model_name:
                    # self.yolo2main_status_msg.emit('Change Model...')
                    self.setup_model(self.new_model_name)
                    self.used_model_name = self.new_model_name

                # pause switch  ç”¨äºæ§åˆ¶ç¨‹åºçš„æš‚åœå’Œç»§ç»­
                if self.continue_dtc:
                    # time.sleep(0.001)
                    self.yolo2main_status_msg.emit('Detecting...')
                    batch = next(self.dataset)  # next data

                    self.batch = batch
                    path, im, im0s, vid_cap, s = batch
                    visualize = increment_path(self.save_dir / Path(path).stem,
                                               mkdir=True) if self.args.visualize else False

                    # Calculation completion and frame rate (to be optimized)
                    count += 1  # frame count +1
                    if vid_cap:
                        all_count = vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)  # total frames
                    else:
                        all_count = 1
                    self.progress_value = int(count / all_count * 1000)  # progress bar(0~1000)
                    if count % 5 == 0 and count >= 5:  # Calculate the frame rate every 5 frames
                        self.yolo2main_fps.emit(str(int(5 / (time.time() - start_time))))
                        start_time = time.time()

                    # preprocess
                    # self.dt åŒ…å«äº†ä¸‰ä¸ª DetectorTime ç±»å‹çš„å¯¹è±¡ï¼Œè¡¨ç¤ºé¢„å¤„ç†ã€æ¨ç†å’Œåå¤„ç†æ‰€èŠ±è´¹çš„æ—¶é—´

                    ## ä½¿ç”¨ with è¯­å¥è®°å½•ä¸‹ä¸‹ä¸€è¡Œä»£ç æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œself.dt[0] è¡¨ç¤ºè®°å½•é¢„å¤„ç†æ“ä½œæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚
                    with self.dt[0]:
                        # è°ƒç”¨ self.preprocess æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å¤„ç†åçš„å›¾åƒèµ‹å€¼ç»™ im å˜é‡ã€‚
                        im = self.preprocess(im)
                        # å¦‚æœ im çš„ç»´åº¦ä¸º 3ï¼ˆRGB å›¾åƒï¼‰ï¼Œåˆ™è¡¨ç¤ºè¿™æ˜¯ä¸€å¼ å•å¼ å›¾åƒï¼Œéœ€è¦å°†å…¶æ‰©å±•æˆ 4 ç»´ï¼ŒåŠ ä¸Š batch ç»´åº¦ã€‚
                        if len(im.shape) == 3:
                            im = im[None]  # expand for batch dim  æ‰©å¤§æ‰¹é‡è°ƒæš—
                    # inference
                    with self.dt[1]:
                        # è°ƒç”¨æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œæ¨ç†ï¼Œå¹¶å°†ç»“æœèµ‹å€¼ç»™ preds å˜é‡ã€‚
                        preds = self.model(im, augment=self.args.augment, visualize=visualize)
                    # postprocess
                    with self.dt[2]:
                        # è°ƒç”¨ self.postprocess æ–¹æ³•å¯¹æ¨ç†ç»“æœè¿›è¡Œåå¤„ç†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° self.results å˜é‡ä¸­ã€‚
                        # å…¶ä¸­ preds æ˜¯æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œim æ˜¯æ¨¡å‹è¾“å…¥çš„å›¾åƒï¼Œè€Œ im0s æ˜¯åŸå§‹å›¾åƒçš„å¤§å°ã€‚
                        self.results = self.postprocess(preds, im, im0s)



                    # visualize, save, write results
                    n = len(im)  # To be improved: support multiple img
                    for i in range(n):
                        self.results[i].speed = {
                            'preprocess': self.dt[0].dt * 1E3 / n,
                            'inference': self.dt[1].dt * 1E3 / n,
                            'postprocess': self.dt[2].dt * 1E3 / n}
                        p, im0 = (path[i], im0s[i].copy()) if self.source_type.webcam or self.source_type.from_img \
                            else (path, im0s.copy())

                        p = Path(p)  # the source dir

                        # s:::   video 1/1 (6/6557) 'path':
                        # must, to get boxs\labels
                        label_str = self.write_results(i, self.results, (p, im, im0))  # labels   /// original :s +=

                        # labels and nums dict
                        class_nums = 0
                        target_nums = 0
                        self.labels_dict = {}
                        if 'no detections' in label_str:
                            pass
                        else:
                            for ii in label_str.split(',')[:-1]:
                                nums, label_name = ii.split('~')
                                self.labels_dict[label_name] = int(nums)
                                target_nums += int(nums)
                                class_nums += 1

                        # save img or video result
                        if self.save_res:
                            self.save_preds(vid_cap, i, str(self.save_dir / p.name))

                        # Send test results ã€å‘é€ä¿¡å· ç»™ label æ˜¾ç¤ºå›¾åƒã€‘

                        #  emit() æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å¸¸å¸¸ç”¨äºPySide/PyQtä¿¡å·æœºåˆ¶.
                        #  emit() æ–¹æ³•å®ç°äº†ä¿¡å·æœºåˆ¶çš„å‘é€æ“ä½œï¼Œä¹Ÿå°±æ˜¯å‘å…³è”çš„æ§½å‡½æ•°å‘é€ä¿¡å·å€¼.
                        # å½“æˆ‘ä»¬åœ¨ä¸Šè¿° emit() æ–¹æ³•ä¸­ä¼ å…¥äº†ä¸€ä¸ªå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å°±ä¼šè¢«å‘é€ç»™ä¸€ä¸ªæ¥æ”¶æ§½å‡½æ•°å¹¶å¤„ç†å®ƒã€‚
                        # emit() æ˜¯ä¸€ä¸ªåŒæ­¥æ“ä½œ, å®ƒåœ¨ä¿¡å·å‘å‡ºåä¼šç›´æ¥è¿›å…¥åˆ°ä¸ä¹‹è¿æ¥çš„æ§½å‡½æ•°
                        # è€Œä¸æ˜¯åƒå¤šçº¿ç¨‹ä¸€æ ·ç­‰å¾…è¢«æ‰§è¡Œã€‚æ‰€ä»¥ å¯ä»¥è¯´emitä¸æ˜¯å¼‚æ­¥çš„ã€‚

                        # åœ¨ PySide6 æˆ– PyQt6 ä¸­ï¼Œemit() å‘å‡ºçš„ä¿¡å·è°ƒç”¨å°†ä¼šå¼‚æ­¥åœ°å°†ä¿¡å·æ”¾å…¥äº‹ä»¶é˜Ÿåˆ—é‡Œï¼Œ
                        # ä¹‹ååœ¨äº‹ä»¶å¾ªç¯ä¸­è¿›è¡Œå¤„ç†ï¼Œå¦‚æœè¿™ä¸ªä¿¡å·ä¸å¤šä¸ªæ§½å‡½æ•°è¿æ¥ï¼Œ
                        # é‚£ä¹ˆè¿™äº›æ§½å‡½æ•°å°†ä¼šæŒ‰å…ˆåé¡ºåºè¢«å¼‚æ­¥åœ°è°ƒç”¨ã€‚
                        # ä¹Ÿå°±æ˜¯è¯´ï¼Œè™½ç„¶ emit() æ“ä½œæœ¬èº«æ˜¯åŒæ­¥çš„ï¼Œä½†æ§½å‡½æ•°çš„è§¦å‘æ˜¯å¼‚æ­¥çš„ã€‚
                        #
                        # éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœæ‚¨åœ¨ä¸»çº¿ç¨‹ä¸­è°ƒç”¨ emit()ï¼Œé‚£ä¹ˆæ§½å‡½æ•°å°†ä¼šåœ¨ä¸»çº¿ç¨‹ä¸­è¢«å¼‚æ­¥åœ°è°ƒç”¨ï¼›
                        # å¦‚æœæ‚¨åœ¨éä¸»çº¿ç¨‹ä¸­è°ƒç”¨ emit()ï¼Œé‚£ä¹ˆæ§½å‡½æ•°å°†ä¼šåœ¨ä¸è¯¥å­çº¿ç¨‹ç›¸å¯¹åº”çš„ä¸»çº¿ç¨‹ä¸­è¢«å¼‚æ­¥åœ°è°ƒç”¨ï¼Œ
                        # è¿™æ˜¯ç”±äº PySide6 æˆ– PyQt6 çš„çº¿ç¨‹æ¨¡å‹æ‰€å†³å®šçš„ã€‚
                        self.yolo2main_res_img.emit(im0)  # after detection  ----------ç»“æœ
                        self.yolo2main_pre_img.emit(im0s if isinstance(im0s, np.ndarray) else im0s[0])  # Before testing
                        # self.yolo2main_labels.emit(self.labels_dict)        # webcam need to change the def write_results
                        self.yolo2main_class_num.emit(class_nums)
                        self.yolo2main_target_num.emit(target_nums)

                        print('send success!')

                        # if self.speed_thres != 0:
                        #     time.sleep(self.speed_thres / 1)  # delay , ms

                    self.yolo2main_progress.emit(self.progress_value)  # progress bar

                # Detection completed
                if count + 1 >= all_count:
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # release final video writer
                    self.yolo2main_status_msg.emit('Detection completed')
                    break

        except Exception as e:
            print(e)
            self.yolo2main_status_msg.emit('%s' % e)

    def get_annotator(self, img):
        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))

    def preprocess(self, img):
        img = torch.from_numpy(img).to(self.model.device)
        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
        img /= 255  # 0 - 255 to 0.0 - 1.0
        return img

    def postprocess(self, preds, img, orig_img):
        ### important
        preds = ops.non_max_suppression(preds,
                                        self.conf_thres,
                                        self.iou_thres,
                                        agnostic=self.args.agnostic_nms,
                                        max_det=self.args.max_det,
                                        classes=self.args.classes)

        results = []
        for i, pred in enumerate(preds):
            orig_img = orig_img[i] if isinstance(orig_img, list) else orig_img
            shape = orig_img.shape
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()
            path, _, _, _, _ = self.batch
            img_path = path[i] if isinstance(path, list) else path
            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, boxes=pred))
        # print(results)
        return results

    # ç”»ç»“æœ
    def write_results(self, idx, results, batch):
        # idx æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œç”¨äºæŒ‡å®šæ‰¹å¤„ç†ä¸­çš„æŸä¸ªå›¾åƒï¼›
        # results æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«åœ¨æ¨¡å‹ä¸­å¯¹ batch æ‰§è¡Œå‰å‘ä¼ é€’çš„ç»“æœï¼›
        # batch æ˜¯æ¨¡å‹è¾“å…¥çš„ä¸€ä¸ªæ‰¹å¤„ç†å¼ é‡ï¼Œå®ƒç”±ä¸‰ä¸ªå…ƒç´ ç»„æˆï¼špï¼Œim å’Œ im0ã€‚

        # å°†å…ƒç»„ batch ä¸­ä¸‰ä¸ªå˜é‡è§£åŒ…å­˜å‚¨åˆ° pã€im å’Œ im0 ä¸­ã€‚
        p, im, im0 = batch
        log_string = ''

        # ç”¨äºåˆ¤æ–­è¾“å…¥å›¾åƒçš„å½¢çŠ¶ï¼ˆshapeï¼‰æ˜¯å¦ä¸º 3Dï¼Œå¦‚æœæ˜¯åˆ™åœ¨å‰é¢æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œä»¥è¡¨ç¤ºæ‰¹å¤„ç†ã€‚è¿™æ˜¯ä¸€ç§å¤„ç†ä¸åŒå¤§å°çš„è¾“å…¥å›¾åƒçš„å¸¸è§æ–¹æ³•ã€‚
        if len(im.shape) == 3:
            im = im[None]  # expand for batch dim
        self.seen += 1

        # ä½¿ç”¨ if/else è¯­å¥è®¾ç½®å˜é‡ imcï¼Œè¯¥å˜é‡å­˜å‚¨è¾“å…¥å›¾åƒçš„ä¸€ä¸ªå‰¯æœ¬ã€‚
        # å¦‚æœå‚æ•° save_cropï¼ˆåœ¨ self.args ä¸­ï¼‰ä¸ºçœŸï¼Œåˆ™å­˜å‚¨è£å‰ªçš„å›¾åƒï¼Œå¦åˆ™å­˜å‚¨åŸå§‹å›¾åƒã€‚
        imc = im0.copy() if self.args.save_crop else im0

        # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦æ¥è‡ªç½‘ç»œæ‘„åƒå¤´æˆ–å›¾åƒæ–‡ä»¶
        if self.source_type.webcam or self.source_type.from_img:  # batch_size >= 1         # attention
            log_string += f'{idx}: '
            frame = self.dataset.count
        else:
            frame = getattr(self.dataset, 'frame', 0)
        self.data_path = p
        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')
        # log_string += '%gx%g ' % im.shape[2:]         # !!! don't add img size~
        self.annotator = self.get_annotator(im0)

        # ç»Ÿè®¡æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°é‡å’Œç§ç±»çš„æ®µè½
        det = results[idx].boxes  # TODO: make boxes inherit from tensors

        if len(det) == 0:
            return f'{log_string}(no detections), '  # if no, send this~~

        # æ·»åŠ å½“å‰ç›®æ ‡æ•°é‡å’Œåç§°åˆ°æ—¥å¿—å­—ç¬¦ä¸²
        # ã€det.cls.unique() æ–¹æ³•è¿”å›äº† det.cls åˆ—ä¸­çš„æ‰€æœ‰å”¯ä¸€å€¼ã€‘
        for c in det.cls.unique():
            # det.cls == c è¿™ä¸ªæ¡ä»¶åˆ¤æ–­è¡¨è¾¾å¼ä¼šè¿”å›ä¸€ä¸ªç”±å¸ƒå°”å€¼ç»„æˆçš„æ•°ç»„
            n = (det.cls == c).sum()  # detections per class

            # it only recognizes license-plates and records the total number of license-plates
            if(self.model.names[int(c)] == 'license-plate'):
                log_string = f"{n}~{self.model.names[int(c)]},"  # {'s' * (n > 1)}, "   # don't add 's'

        # now log_string is the classes ğŸ‘†
        # print(log_string)

        # write & save & draw
        for d in reversed(det):

            cls, conf = d.cls.squeeze(), d.conf.squeeze()

            # è·å–ç±»åˆ«  get category
            c = int(cls)  # integer class
            name = f'id:{int(d.id.item())} {self.model.names[c]}' if d.id is not None else self.model.names[c]

            # å¦‚æœä¸æ˜¯è½¦ç‰Œï¼Œåˆ™è·³è¿‡ï¼
            # if there is not a license-plate, jump it
            if (name != 'license-plate'):
                continue

            # ç”»è½¦ç‰Œ draw a license plate
            plate = de_lpr(d.xyxy.squeeze(), im0)
            plate = np.array(plate)
            car_number_laber = ""
            for i in range(0, plate.shape[1]):
                b = CHARS[plate[0][i]]
                car_number_laber += b

            if self.save_txt:  # Write to file å†™å…¥æ–‡æœ¬æ–‡ä»¶

                line = (cls, *(d.xywhn.view(-1).tolist()), conf) \
                    if self.args.save_conf else (cls, *(d.xywhn.view(-1).tolist()))  # label format
                with open(f'{self.txt_path}.txt', 'a') as f:
                    f.write(('%g ' * len(line)).rstrip() % line + '\n')

            # æ£€æµ‹ç»“æœç»˜åˆ¶åˆ°å›¾åƒä¸Šï¼Œå¹¶æ˜¾ç¤ºå‡ºæ¥ã€‚
            if self.save_res or self.args.save_crop or self.args.show or True:  # Add bbox to image(must)

                # å¦‚æœ self.args.hide_labels = Trueï¼Œåˆ™ä¸º None
                # å¦åˆ™ (name if self.args.hide_conf else f'{name} {conf:.2f}')
                # å¦‚æœ self.args.hide_conf = Trueï¼Œåˆ™ä¸º name
                # å¦åˆ™ f'{name} {conf:.2f}'
                self.annotator.box_label(d.xyxy.squeeze(), car_number_laber, color=colors(c, True))

                # åŸæ ‡ç­¾ original label
                # label = None if self.args.hide_labels else (name if self.args.hide_conf else f'{name} {conf:.2f}')


            # å°†ç”»åœ¨å›¾åƒä¸Šçš„è¾¹ç•Œæ¡†åŒºåŸŸä¿å­˜ä¸ºä¸€ä¸ªå•ç‹¬çš„å›¾åƒæˆ–è€…è§†é¢‘æ–‡ä»¶
            if self.args.save_crop:
                save_one_box(d.xyxy,
                             imc,
                             file=self.save_dir / 'crops' / self.model.model.names[c] / f'{self.data_path.stem}.jpg',
                             BGR=True)

        return log_string
